{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498474bf-9913-410c-8d18-2a0d0c7590e4",
   "metadata": {},
   "source": [
    "### Implementing Batch Normalization\n",
    "\n",
    "Reference:\n",
    "1. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf) (The original paper)\n",
    "2. [How does batch normalization help optimization](https://arxiv.org/pdf/1805.11604.pdf) (Useful to understand a bit more about this topic)\n",
    "\n",
    "During the learning process, the input distribution to each layer changes. This makes the training procedure difficult. This paper addresses the problem of internal covariate shift by normalizing the input to each layer.\n",
    "\n",
    "In this notebook, we implement the batch normalization layer and compare two network (one with batchnorm and one w/o) trained on the mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "884e6c59-abc3-4866-87cd-f1f271d0e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops import nn_ops\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from datasets.mnist import MNIST_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12f63399",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e1b9c",
   "metadata": {},
   "source": [
    "Hyper-parameter batch_size is set based on section 4.1 of [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd02aa98-14c6-4a86-a7ad-dcb8a8f63035",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "SHUFFLE_BUFFER_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052aa8a3",
   "metadata": {},
   "source": [
    "Dataset is read from local-disk as numpy arrays and converted to tf.data.Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ad70712-ff2e-4e49-879c-1a1e900e0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/Users/rohit/Desktop/datasets/mnist'\n",
    "x_train, y_train, x_test, y_test = MNIST_DATASET.load_dataset(dataset_path=dataset_path, reshape=False)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a4dd5",
   "metadata": {},
   "source": [
    "We use a simple fully-connected network with sigmoid activations. The input is 28x28 image as a (784, ) vector. We train it for 50 epochs ie. 50k iterations and plot graphs for accuracy and loss using tensorboard. The choice for RMSprop as the optimizer is a random pick. One can try with Adam, SGD etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "598e868c-1062-4fb3-b547-f4781c25bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 99,710\n",
      "Trainable params: 99,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_one = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='sigmoid', input_shape=(784, )),\n",
    "    tf.keras.layers.Dense(100, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(100, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10, )\n",
    "])\n",
    "\n",
    "model_one.build()\n",
    "\n",
    "print(model_one.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4f65218-46dd-46d4-be2b-cdfabadf28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1385a69-ccc9-4e99-ab3e-e9906d3f8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"batch_norm-%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model_one.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325bb66b",
   "metadata": {},
   "source": [
    "We implement the batch norm layer as per Algorithm 1 in [1] shown below\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/batchnorm.1.png\" width=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acff0007-6874-4ee7-a96b-373666d6073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(\n",
    "                                name='gamma',\n",
    "                                shape=(input_shape[-1], ), \n",
    "                                initializer='ones',\n",
    "                                trainable=True)\n",
    "\n",
    "        self.beta = self.add_weight(\n",
    "                                name='beta',\n",
    "                                shape=(input_shape[-1], ), \n",
    "                                initializer='zeros',\n",
    "                                trainable=True)\n",
    "\n",
    "        self.epsilon = 10e-6\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean = tf.math.reduce_mean(inputs, axis=0)\n",
    "        variance = tf.math.reduce_mean(tf.math.square(tf.math.subtract(inputs, mean)), axis=0)\n",
    "\n",
    "        normalized_input = tf.math.divide(tf.math.subtract(inputs, mean), tf.math.sqrt(variance + self.epsilon))\n",
    "\n",
    "        return tf.math.add(tf.math.multiply(self.gamma, normalized_input), self.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8870d5a",
   "metadata": {},
   "source": [
    "This is very much similar to the first model except that we have added the batch norm layer. The activation is applied after this. Rest of the hyper-parameters are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cabe24b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 100)               78500     \n",
      "                                                                 \n",
      " batch_norm1d_6 (BatchNorm1D  (None, 100)              200       \n",
      " )                                                               \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_norm1d_7 (BatchNorm1D  (None, 100)              200       \n",
      " )                                                               \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_norm1d_8 (BatchNorm1D  (None, 100)              200       \n",
      " )                                                               \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100,310\n",
      "Trainable params: 100,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "model_two = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, input_shape=(784, )),\n",
    "    BatchNorm1D(),\n",
    "    tf.keras.layers.Activation(activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    BatchNorm1D(),\n",
    "    tf.keras.layers.Activation(activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    BatchNorm1D(),\n",
    "    tf.keras.layers.Activation(activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10, )\n",
    "])\n",
    "\n",
    "model_two.build()\n",
    "\n",
    "print(model_two.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2844d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_two.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"batch_norm-%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model_two.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ae533",
   "metadata": {},
   "source": [
    "To understand the difference between the two models trained, we look at the tensorboard graphs. \n",
    "\n",
    "First we compare the test accuracy. Red is network with batch norm and green is w/o. We can see that network with batch norm trains faster and overall achieves a higher accuracy. This is consistent with observations in Figure 1 (a) of [1].\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/batchnorm.2.png\" width=\"800\" />\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1e2dee48b02031918904c233009e6681485f7318eff0019dcda8e435e03498a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
